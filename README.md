# CWC
Calibrate with Confidence
These files contain the code for the Calibrate with Confidence method and data for two case studies.


Frequently, a set of objects has to be evaluated by a panel of assessors, but not every object is assessed by each assessor and different assessors  may have different standards or use the marking scale differently, which we term “assessor bias”. 
In addition to assessor bias, the levels of confidence in each assessment may vary according to the match in expertise between assessor and object and the stringency of assessment.

Examples include the evaluation of: grant proposals submitted to a funding panel; candidates for appointment to a lectureship;  students' performance in an options system where they choose to take examinations from a list of options; the quality of research submitted to national research assessment exercise.

A problem facing such panels is how to account for different levels of bias and confidence amongst panel members. 
We propose a method to achieve robust calibration assessors and hence robust scores for outputs. 
It is based upon the premise that we may infer something about assessors' relative standards by examining discrepancies between their evaluations when their evaluations have objects in common,  also taking into account possible differences in declared confidences in their evaluations.

We have submitted a paper to PLOS ONE. 
Our algorithm is described in detail in that paper which is titled "Calibration with confidence: A robust and efficient method for panel assessment" and authored by R.S. MacKay, S. Parker, R. Kenna & R.J. Low.

A preprint version of the paper is available at http://arxiv.org/abs/1504.00340
Our algorithm is fully described in that preprint too.
There is an associated website: http://www.calibratewithconfidence.co.uk
There, the data of relevance for the paper and associated code are also available.
Please feel free to download and test our algorithm which has been designed to run on both Windows  and Unix based operating systems via Python and Microsoft Excel macro packages. 
In order to test and improve our algorithm we would be very grateful if you could please leave feedback on your findings via http://www.calibratewithconfidence.co.uk.


Additionally, and in order to satisfy PLOS ONE requirements that data and algorithm be deposited in a stable, public repository so that the data are permanently accessible, we store the data and code here on github.

The two files here contain the excel code for the Calibrate with Confidence method and data for two case studies.

The file Casestudy1.xlsx contains the averaged results generated by multiple simulations using the algorithm described in the paper.
These are the data reported in Case Study 1 of the paper. 

The file CaseStudy2.xlsm contains the raw (real) data and processed data associated with Case Study 2.
It also contains the data generated by application of the algorithm.
These are the data reported upon in the paper.
The notation is explained in the paper.



We welcome questions and feedback.
